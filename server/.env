## Hugging Face text-embeddings-inference (TEI) settings
# TEI_VERSION=cpu-1.7  # CPU
TEI_VERSION=turing-1.7  # GPU T4
MODEL_ID=hanwenzhu/all-distilroberta-v1-lr2e-4-bs256-nneg3-ml-ne5-may07
# **TODO**: see if lower precision hurts performance and/or increases throughput
DTYPE=float32  # float32, float16, bfloat16

# NB: If I understand correctly, a "client batch"/"request" is a single request
# (from FastAPI to TEI) with maximum `MAX_CLIENT_BATCH_SIZE` inputs and hence
#   MAX_CLIENT_BATCH_SIZE * (max-seq-len = 512) tokens
# to embed. TEI groups concurrent "client batches" into "batches",
# each with maximum
#   MAX_BATCH_TOKENS tokens
# to send to GPU.

# Per TEI docs, this should be largest possible until the model is compute bound.
# The number below can probably still be increased on a T4 GPU.
MAX_BATCH_TOKENS=131072  # default: 16384
# This is the default value; only for communication between TEI and FastAPI.
MAX_CLIENT_BATCH_SIZE=32  # default: 32
# Per TEI docs, additional requests beyond this will be refused,
# but I couldn't achieve this; instead, requests just take longer
# to even get in the queue (**TODO** why?).
# Therefore, I leave it at default, and control the max concurrency
# on the FastAPI side by `EMBED_SERVICE_MAX_CONCURRENT_INPUTS`.
# (If I understand correctly, this allows a maximum of
#   MAX_CONCURRENT_REQUESTS * batch-size * (max-seq-len = 512)
# concurrent inputs in the queue, but see above.)
MAX_CONCURRENT_REQUESTS=512  # default: 512

## My settings for interaction between FastAPI and TEI
# Timeout for FastAPI to wait for TEI; this is now set to "" i.e. no timeout.
# (Rationale: rate-limiting should be done before the requests are even sent to TEI,
# by controlling `EMBED_SERVICE_MAX_CONCURRENT_INPUTS`.)
EMBED_SERVICE_TIMEOUT=
# The maximum number of concurrent embedding inputs for TEI processing.
# Any calls to `/retrieve` that need more inputs beyond this
# will result in 500 Internal Server Error.
# (Recall that most embeddings are cache hits, only cache misses count toward this.)
# This hopefully stops malicious actors from overflooding the server
# deliberately with new premises that are cache misses.
# Also, this should be â‰¥ 1 + MAX_NEW_PREMISES.
# (*The code also assumes there is a single uvicorn worker.*)
EMBED_SERVICE_MAX_CONCURRENT_INPUTS=4096

## FastAPI settings
# Size of the LRU cache for embeddings.
# The size of the cache will be
#   LRU_CACHE_SIZE * (model-width = 768) * sizeof(DTYPE = float32) = 384 MB
LRU_CACHE_SIZE=131072
# (Thomas) With the LRU cache, I think the primary bottleneck
# for increasing this number is on the user side:
# the time it takes to pretty-print all the new statements.
# For 2048, this is ~4 seconds.
# We also need to take into account the possibility for a user to
# (maliciously) overflood the compute / cache constantly,
# with MAX_NEW_PREMISES new premises in each request.
MAX_NEW_PREMISES=2048
MAX_K=1024
