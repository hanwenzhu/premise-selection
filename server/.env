# Hugging Face text-embeddings-inference settings
# TEI_VERSION=cpu-1.7  # CPU
TEI_VERSION=turing-1.7  # GPU T4
MODEL_ID=hanwenzhu/all-distilroberta-v1-lr2e-4-bs256-nneg3-ml-ne5-may07
DTYPE=float32  # float16 or float32
MAX_BATCH_TOKENS=16384  # **TODO**, main thing to tune?
MAX_CLIENT_BATCH_SIZE=32
EMBED_SERVICE_TIMEOUT=60
MAX_CONCURRENT_REQUESTS=512  # this is only for the inference service (not app frontend)

# **TODO** tune this number
# (Thomas) With the LRU cache, I think the primary bottleneck
# for increasing this number is on the user side:
# the time it takes to pretty-print all the new statements.
# For 2048, this is ~4 seconds
MAX_NEW_PREMISES=2048
MAX_K=1024
