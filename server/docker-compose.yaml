services:
  embed:
    image: ghcr.io/huggingface/text-embeddings-inference:${TEI_VERSION}
    ports:
      - "8080:80"
    environment:
      - DTYPE=${DTYPE}
      - MODEL_ID=${MODEL_ID}
      - MAX_BATCH_TOKENS=${MAX_BATCH_TOKENS}
      - MAX_CLIENT_BATCH_SIZE=${MAX_CLIENT_BATCH_SIZE}
      - MAX_CONCURRENT_REQUESTS=${MAX_CONCURRENT_REQUESTS}
    volumes:
      - ./data:/data
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  app:
    build: ./app
    ports:
      - "0.0.0.0:80:8000"
    depends_on:
      - embed
    environment:
      - DATA_DIR=/data
      - PRECOMPUTED_EMBEDDINGS_PATH=/data/embeddings.npy
      - EMBED_SERVICE_URL=http://embed:80
      - EMBED_SERVICE_TIMEOUT=${EMBED_SERVICE_TIMEOUT}
      - EMBED_SERVICE_MAX_CONCURRENT_INPUTS=${EMBED_SERVICE_MAX_CONCURRENT_INPUTS}
      - LRU_CACHE_SIZE=${LRU_CACHE_SIZE}
      - MAX_NEW_PREMISES=${MAX_NEW_PREMISES}
      - MAX_CLIENT_BATCH_SIZE=${MAX_CLIENT_BATCH_SIZE}
      - MAX_K=${MAX_K}
      - DTYPE=${DTYPE}
    volumes:
      - ./app:/app
      - ./data:/data
